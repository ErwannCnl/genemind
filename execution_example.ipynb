{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f860d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9d6d903",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_core'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Optional\n",
      "\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PydanticOutputParser\n",
      "\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n",
      "\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_chat_model\n",
      "\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_core'"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import init_chat_model\n",
    "from src.pydantic_classes import StudyExtraction, Standard_gene, Cancer_gene\n",
    "\n",
    "# Initialise LLM (Gemini 2.5)\n",
    "llm = init_chat_model(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    model_provider=\"google_genai\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# Canonical user input\n",
    "paragraph = (\n",
    "    \"I identified these five genes to be significantly more mutated than expected by chance in my cohort of human brain cancer patients: TP53, AKT3, EGFR, ATRX and PDX1.\"\n",
    ")\n",
    "\n",
    "#alternative implementation to parse as pydantic more robustly\n",
    "parser = PydanticOutputParser(pydantic_object=StudyExtraction)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Extract per schema:\\n{format_instructions}\"),\n",
    "    (\"human\", \"{paragraph}\"),\n",
    "]).partial(format_instructions=format_instructions)\n",
    "parsing_llm = prompt | llm | parser\n",
    "\n",
    "# pass raw user input \"paragraph\"\n",
    "parsed_input = parsing_llm.invoke({\"paragraph\": paragraph})\n",
    "#convert to JSON string\n",
    "json_output = parsed_input.model_dump_json(indent=2)\n",
    "print(json_output)\n",
    "\n",
    "#next step "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genemind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
